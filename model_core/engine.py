import torch
from torch.distributions import Categorical
from tqdm import tqdm
import json
from model_core.factors import FeatureEngineer
from model_core.ops import OPS_CONFIG
from model_core.config import ModelConfig
from model_core.data_loader_csv import CsvCryptoDataLoader, CsvLoaderConfig
from model_core.data_loader import CryptoDataLoader
from model_core.alphagpt import AlphaGPT, NewtonSchulzLowRankDecay, StableRankMonitor
from model_core.vm import StackVM
from model_core.backtest import MemeBacktest

class AlphaEngine:
    def __init__(self, use_lord_regularization=True, lord_decay_rate=1e-3, lord_num_iterations=5):
        """
        Initialize AlphaGPT training engine.
        
        Args:
            use_lord_regularization: Enable Low-Rank Decay (LoRD) regularization
            lord_decay_rate: Strength of LoRD regularization
            lord_num_iterations: Number of Newton-Schulz iterations per step
        """
        '''self.loader = CryptoDataLoader()
        self.loader.load_data()'''
        cfg = CsvLoaderConfig(
            csv_paths=["./data/your_klines.csv"],  # æ”¹æˆä½ çš„è·¯å¾„
            device=ModelConfig.DEVICE,
            max_symbols=50,  # ä½ å¯ä»¥å…ˆå°ä¸€ç‚¹è¯•è·‘
            liquidity_mode="quote_volume",
        )
        self.loader = CsvCryptoDataLoader(cfg).load_data()
        
        self.model = AlphaGPT().to(ModelConfig.DEVICE)
        expected_vocab = FeatureEngineer.INPUT_DIM + len(OPS_CONFIG)
        if getattr(self.model, "vocab_size", None) != expected_vocab:
            raise ValueError(
                f"Vocab mismatch: model.vocab_size={self.model.vocab_size}, "
                f"but expected={expected_vocab} (FeatureEngineer.INPUT_DIM + len(OPS_CONFIG))"
            )
        
        # Standard optimizer
        self.opt = torch.optim.AdamW(self.model.parameters(), lr=1e-3)
        
        # Low-Rank Decay regularizer
        self.use_lord = use_lord_regularization
        if self.use_lord:
            self.lord_opt = NewtonSchulzLowRankDecay(
                self.model.named_parameters(),
                decay_rate=lord_decay_rate,
                num_iterations=lord_num_iterations,
                target_keywords=["q_proj", "k_proj", "attention", "qk_norm"]
            )
            self.rank_monitor = StableRankMonitor(
                self.model,
                target_keywords=["q_proj", "k_proj"]
            )
        else:
            self.lord_opt = None
            self.rank_monitor = None
        
        self.vm = StackVM()
        self.bt = MemeBacktest()
        
        self.best_score = -float('inf')
        self.best_formula = None
        self.training_history = {
            'step': [],
            'avg_reward': [],
            'best_score': [],
            'stable_rank': []
        }

        self.feat_offset = FeatureEngineer.INPUT_DIM
        self.vocab_size = self.feat_offset + len(OPS_CONFIG)
        # æ¯ä¸ª token çš„ arityï¼šfeature=0ï¼Œop=1/2/3...
        self.arity_vec = torch.zeros(self.vocab_size, dtype=torch.long, device=ModelConfig.DEVICE)
        for j, (_, _, arity) in enumerate(OPS_CONFIG):
            self.arity_vec[self.feat_offset + j] = arity
        # å•æ­¥æœ€å¤§â€œé™æ ˆå¹…åº¦â€ï¼šmax(arity-1)ï¼Œç”¨äºåˆ¤æ–­â€œå‰©ä½™æ­¥æ•°æ˜¯å¦è¿˜èƒ½æ”¶æ•›åˆ° 1â€
        self.max_reduce = max((arity - 1 for (_, _, arity) in OPS_CONFIG), default=0)

    def _build_strict_mask_rpn(self, depth: torch.Tensor, step: int) -> torch.Tensor:
        """
        depth: [B] å½“å‰æ ˆæ·±åº¦ï¼ˆstack sizeï¼‰
        è¿”å›: [B, vocab_size] çš„ maskï¼ˆå…è®¸=0ï¼Œç¦æ­¢=-infï¼‰
        """
        B = depth.shape[0]
        V = self.vocab_size
        device = depth.device
        # [B, V]
        arity = self.arity_vec.unsqueeze(0).expand(B, V)
        depth_b = depth.unsqueeze(1).expand(B, V)
        # é€‰æ‹©è¯¥ token åçš„æ ˆæ·±åº¦
        depth_after = torch.where(
            arity == 0,  # feature
            depth_b + 1,
            depth_b - arity + 1  # op: -k + 1
        )
        # æ¡ä»¶1ï¼šä¸èƒ½ underflowï¼ˆé€‰ op æ—¶å¿…é¡» depth >= arityï¼‰
        valid_underflow = (arity == 0) | (depth_b >= arity)
        # æ¡ä»¶2ï¼šå¿…é¡»â€œå¯æ”¶æ•›â€ï¼šå‰©ä½™æ­¥æ•°å†…è‡³å°‘å­˜åœ¨ä¸€ç§æ–¹å¼èƒ½æŠŠ depth_after æ”¶æ•›åˆ° 1
        r_after = ModelConfig.MAX_FORMULA_LEN - (step + 1)
        if r_after == 0:
            finishable = (depth_after == 1)
        elif self.max_reduce <= 0:
            # å¦‚æœæ‰€æœ‰ op éƒ½æ˜¯ unaryï¼ˆarity-1=0ï¼‰ï¼Œæ ˆæ°¸è¿œé™ä¸ä¸‹å»ï¼šå¿…é¡»ä¸€ç›´ä¿æŒ 1
            finishable = (depth_after == 1)
        else:
            # æœ€ä¹è§‚æƒ…å†µä¸‹ï¼Œæ¯ä¸€æ­¥æœ€å¤šæŠŠæ ˆæ·±åº¦å‡å°‘ max_reduce
            # è¦ä» d æ”¶æ•›åˆ° 1ï¼Œè‡³å°‘è¦å‡å°‘ (d-1)
            finishable = (depth_after <= (self.max_reduce * r_after + 1))
        allowed = valid_underflow & finishable
        mask = torch.full((B, V), float("-inf"), device=device, dtype=torch.float32)
        mask.masked_fill_(allowed, 0.0)
        return mask

    def train(self):
        print("ğŸš€ Starting Meme Alpha Mining with LoRD Regularization..." if self.use_lord else "ğŸš€ Starting Meme Alpha Mining...")
        if self.use_lord:
            print(f"   LoRD Regularization enabled")
            print(f"   Target keywords: ['q_proj', 'k_proj', 'attention', 'qk_norm']")
        
        pbar = tqdm(range(ModelConfig.TRAIN_STEPS))
        
        for step in pbar:
            bs = ModelConfig.BATCH_SIZE
            inp = torch.zeros((bs, 1), dtype=torch.long, device=ModelConfig.DEVICE)
            
            log_probs = []
            tokens_list = []

            depth = torch.zeros(bs, dtype=torch.long, device=ModelConfig.DEVICE)
            for step_in_formula in range(ModelConfig.MAX_FORMULA_LEN):
                logits, _, _ = self.model(inp)  # [B, V]
                mask = self._build_strict_mask_rpn(depth, step_in_formula)  # [B, V]
                dist = Categorical(logits=logits + mask)
                action = dist.sample()
                log_probs.append(dist.log_prob(action))
                tokens_list.append(action)
                inp = torch.cat([inp, action.unsqueeze(1)], dim=1)
                # æ›´æ–°æ ˆæ·±åº¦
                arity = self.arity_vec[action]  # [B]
                is_feat = (arity == 0)
                depth = torch.where(is_feat, depth + 1, depth - arity + 1)
            
            seqs = torch.stack(tokens_list, dim=1)
            
            rewards = torch.zeros(bs, device=ModelConfig.DEVICE)
            invalid = 0
            for i in range(bs):
                res = self.vm.execute(seqs[i].tolist(), self.loader.feat_tensor)
                if res is None:
                    invalid += 1
                formula = seqs[i].tolist()
                
                res = self.vm.execute(formula, self.loader.feat_tensor)
                
                if res is None:
                    rewards[i] = -5.0
                    continue
                
                if res.std() < 1e-4:
                    rewards[i] = -2.0
                    continue
                
                score, ret_val = self.bt.evaluate(res, self.loader.raw_data_cache, self.loader.target_ret)
                rewards[i] = score
                
                if score.item() > self.best_score:
                    self.best_score = score.item()
                    self.best_formula = formula
                    tqdm.write(f"[!] New King: Score {score:.2f} | Ret {ret_val:.2%} | Formula {formula}")
            tqdm.write(f"InvalidRatio={invalid / bs:.2%}")
            # Normalize rewards
            adv = (rewards - rewards.mean()) / (rewards.std() + 1e-5)
            
            loss = 0
            for t in range(len(log_probs)):
                loss += -log_probs[t] * adv
            
            loss = loss.mean()
            
            # Gradient step
            self.opt.zero_grad()
            loss.backward()
            self.opt.step()
            
            # Apply Low-Rank Decay regularization
            if self.use_lord:
                self.lord_opt.step()
            
            # Logging
            avg_reward = rewards.mean().item()
            postfix_dict = {'AvgRew': f"{avg_reward:.3f}", 'BestScore': f"{self.best_score:.3f}"}
            
            if self.use_lord and step % 100 == 0:
                stable_rank = self.rank_monitor.compute()
                postfix_dict['Rank'] = f"{stable_rank:.2f}"
                self.training_history['stable_rank'].append(stable_rank)
            
            self.training_history['step'].append(step)
            self.training_history['avg_reward'].append(avg_reward)
            self.training_history['best_score'].append(self.best_score)
            
            pbar.set_postfix(postfix_dict)

        # Save best formula
        with open("best_meme_strategy.json", "w") as f:
            json.dump(self.best_formula, f)
        
        # Save training history
        import json as js
        with open("training_history.json", "w") as f:
            js.dump(self.training_history, f)
        
        print(f"\nâœ“ Training completed!")
        print(f"  Best score: {self.best_score:.4f}")
        print(f"  Best formula: {self.best_formula}")


if __name__ == "__main__":
    eng = AlphaEngine(use_lord_regularization=True)
    eng.train()